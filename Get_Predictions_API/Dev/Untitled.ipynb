{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eba042a-e636-4888-af46-32d2b9c38544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from google.cloud import bigquery\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04cb9b7d-fe34-4373-a7be-4bdbf4c0a9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your GCS bucket name\n",
    "bucket_name = 'ccai-storage'\n",
    "\n",
    "# Paths to the Parquet files in your GCS bucket\n",
    "articles_parquet_path = 'make_prediction/test_file.parquet'\n",
    "\n",
    "# Initialize the GCS client\n",
    "client = storage.Client()\n",
    "\n",
    "# Function to download a Parquet file from GCS and load it into a pandas DataFrame\n",
    "def load_parquet_from_gcs(bucket_name, file_path):\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    byte_stream = BytesIO()\n",
    "    blob.download_to_file(byte_stream)\n",
    "    byte_stream.seek(0)\n",
    "    df = pd.read_parquet(byte_stream, engine='pyarrow')\n",
    "    return df\n",
    "\n",
    "# Load the Parquet files\n",
    "articles_df = load_parquet_from_gcs(bucket_name, articles_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db16ad13-7834-4e29-b930-4d1a9f7593e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: uri\n",
      "7892121222\n",
      "\n",
      "\n",
      "Column: url\n",
      "https://www.ilfattoquotidiano.it/2023/12/18/auto-si-schianta-contro-suv-della-scorta-di-biden-il-tonfo-e-la-sorpresa-del-presidente-video/7386540/\n",
      "\n",
      "\n",
      "Column: title\n",
      "Auto si schianta contro suv della scorta di Biden: il tonfo e la sorpresa del presidente - Video - Il Fatto Quotidiano\n",
      "\n",
      "\n",
      "Column: body\n",
      "Al momento dell'impatto di un'auto contro un suv del corteo presidenziale, Joe Biden stava uscendo da un evento elettorale al quartier generale della campagna per il 2024 a Wilmington. Lo riferisce la Casa Bianca in una nota. Nel momento in cui l'auto, una berlina argentata, ha colpito un suv del corteo gli uomini del Secret Service l'hanno circondata con le pistole puntate e il conducente ha alzato le mani. Il presidente ha assistito alla scena con un'espressione sorpresa e poi Ã¨ stato scortato alla sua auto e quindi nella sua residenza in Delaware, riferiscono i giornalisti al seguito.\n",
      "\n",
      "\n",
      "Column: date\n",
      "2023-12-18\n",
      "\n",
      "\n",
      "Column: time\n",
      "08:47:11\n",
      "\n",
      "\n",
      "Column: dateTime\n",
      "2023-12-18T08:47:11Z\n",
      "\n",
      "\n",
      "Column: dateTimePub\n",
      "2023-12-18T08:42:03Z\n",
      "\n",
      "\n",
      "Column: lang\n",
      "ita\n",
      "\n",
      "\n",
      "Column: isDuplicate\n",
      "True\n",
      "\n",
      "\n",
      "Column: dataType\n",
      "news\n",
      "\n",
      "\n",
      "Column: sentiment\n",
      "nan\n",
      "\n",
      "\n",
      "Column: eventUri\n",
      "None\n",
      "\n",
      "\n",
      "Column: relevance\n",
      "1\n",
      "\n",
      "\n",
      "Column: image\n",
      "https://st.ilfattoquotidiano.it/wp-content/uploads/2023/12/18/BidenAuto.jpg\n",
      "\n",
      "\n",
      "Column: authors\n",
      "F. Q.\n",
      "\n",
      "\n",
      "Column: sharesFacebook\n",
      "0\n",
      "\n",
      "\n",
      "Column: sourceTitle\n",
      "Il Fatto Quotidiano\n",
      "\n",
      "\n",
      "Column: sourceLocationLabel\n",
      "Rome\n",
      "\n",
      "\n",
      "Column: categoryLabels\n",
      "news/Politics\n",
      "\n",
      "\n",
      "Column: categoryWeights\n",
      "53\n",
      "\n",
      "\n",
      "Column: importanceRank\n",
      "191788\n",
      "\n",
      "\n",
      "Column: alexaGlobalRank\n",
      "3336\n",
      "\n",
      "\n",
      "Column: alexaCountryRank\n",
      "47\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each column\n",
    "random_row = articles_df.sample()\n",
    "\n",
    "for col in random_row.columns:\n",
    "    print(f\"Column: {col}\")\n",
    "    # Iterate through each row in the column\n",
    "    for val in random_row[col]:\n",
    "        print(val)\n",
    "    print(\"\\n\")  # Add a newline after each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bff02-fe60-47d8-b0a5-a63017560479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3735a1bd-ca53-4913-a20e-265c7653703c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, bucket: str, folder: str, num_doc: int, random_seed: int):\n",
    "        self.bucket = bucket\n",
    "        self.folder = folder\n",
    "        self.num_doc = num_doc\n",
    "        self.random_seed = random_seed\n",
    "        logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "        \n",
    "    def data_preprocessing(self, input_path: str) -> str:\n",
    "        parquet_file_name = input_path\n",
    "        processor = GCSParquetLoader(self.bucket, self.folder, input_path, parquet_file_name, self.num_doc, self.random_seed)\n",
    "        return processor.process()\n",
    "    \n",
    "    def data_tokenization(self, input_path: str) -> str:\n",
    "        file_path = input_path\n",
    "        parquet_file_name = file_path\n",
    "        processor = TokenizationProcessor(self.bucket, file_path, self.folder, parquet_file_name)\n",
    "        return processor.process()\n",
    "    \n",
    "    def data_sentiment(self, input_path: str, text_column: str) -> str:\n",
    "        file_path = input_path\n",
    "        parquet_file_name = file_path\n",
    "        processor = GCSSentimentAnalyzer(self.bucket, file_path, self.folder, parquet_file_name, text_column, self.num_doc)\n",
    "        return processor.process()\n",
    "    \n",
    "    def data_moderate(self, input_path: str, text_column: str) -> str:\n",
    "        file_path = input_path\n",
    "        parquet_file_name = file_path\n",
    "        processor = GCSTextModerationLoader(self.bucket, file_path, self.folder, parquet_file_name, text_column, self.num_doc)\n",
    "        return processor.process()\n",
    "    \n",
    "    def data_entities(self, input_path: str, text_column: str) -> str:\n",
    "        file_path = input_path\n",
    "        parquet_file_name = file_path\n",
    "        processor = GCSCEntityAnalyzer(self.bucket, file_path, self.folder, parquet_file_name, text_column, self.num_doc)\n",
    "        return processor.process()\n",
    "    \n",
    "    def data_bigquery(self, input_path: str, project_id: str, dataname: str, table_id: str, location: str) -> str:\n",
    "        file_path = input_path\n",
    "        parquet_file_name = file_path\n",
    "        processor = GCS_Bigquery(self.bucket, file_path, self.folder, parquet_file_name, project_id, dataname, table_id, location)\n",
    "        return processor.upload_dataframe_to_bigquery()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d287903-ca46-45af-a9c8-98c665a3b774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pipeline_preprocessing import DataProcessor\n",
    "\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "from kfp.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "from typing import NamedTuple\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#import kfp.v2.dsl as dsl\n",
    "#import google_cloud_pipeline_components as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "\n",
    "from google.cloud import bigquery \n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "REGION= \"europe-west3\"\n",
    "REPO_NAME = \"repo-demo3\"\n",
    "SERVICE_ACCOUNT = \"944308723981-compute@developer.gserviceaccount.com\"\n",
    "BUCKET = \"ccai-storage\"\n",
    "PIPELINE_NAME = \"automl_pipeline\"\n",
    "YAML_NAME = f\"{PIPELINE_NAME}.yml\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline_root/\"\n",
    "DISPLAY_NAME = PIPELINE_NAME.replace(\"_\", \"-\")\n",
    "NOTEBOOK = \"automl\"\n",
    "DATANAME = \"datasetnlp\"\n",
    "FILE_PATH = 'test_file.parquet'\n",
    "FOLDER = 'make_prediction'\n",
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "TABLE_ID = \"testdatabq\"\n",
    "TEXT_COLUMN = 'body_pre'\n",
    "LOCATION = \"europe-west3\"\n",
    "NUM_DOC = 20\n",
    "RANDOM_SEED=123\n",
    "#BQ_SOURCE = \"bq://gcp-ccai-auto-ml-contactcenter.datasetnlp.stepfinalbq\"\n",
    "OUTPUT_PROCESSING = 'output_processing.parquet'\n",
    "OUTPUT_TOKENIZATION = 'output_tokenized.parquet'\n",
    "OUTPUT_SENTIMENT = 'output_sentiment.parquet'\n",
    "OUTPUT_MODERATE = 'output_moderate.parquet'\n",
    "OUTPUT_ENTITIES = 'output_entities.parquet'\n",
    "OUTPUT_FINAL = 'step_final_bq.parquet'\n",
    "\n",
    "# Resources\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=PIPELINE_ROOT, location=REGION)\n",
    "bq = bigquery.Client()\n",
    "\n",
    "# Initialize DataProcessor object\n",
    "data_processor = DataProcessor(bucket=BUCKET, folder=FOLDER, num_doc=NUM_DOC, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff50dffc-49d2-4aef-91f4-59b064d9f354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bigquery_upload_status \u001b[38;5;241m=\u001b[39m data_processor\u001b[38;5;241m.\u001b[39mdata_bigquery(file_path\u001b[38;5;241m=\u001b[39m\u001b[43mentity_data_path\u001b[49m, \n\u001b[1;32m      2\u001b[0m                                                       parquet_file_name\u001b[38;5;241m=\u001b[39mOUTPUT_FINAL, \n\u001b[1;32m      3\u001b[0m                                                       project_id\u001b[38;5;241m=\u001b[39mPROJECT_ID, \n\u001b[1;32m      4\u001b[0m                                                       dataname\u001b[38;5;241m=\u001b[39mDATANAME, \n\u001b[1;32m      5\u001b[0m                                                       table_id\u001b[38;5;241m=\u001b[39mTABLE_ID, \n\u001b[1;32m      6\u001b[0m                                                       location\u001b[38;5;241m=\u001b[39mLOCATION)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigQuery Upload completed. Status:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bigquery_upload_status)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "bigquery_upload_status = data_processor.data_bigquery(file_path=entity_data_path, \n",
    "                                                      parquet_file_name=OUTPUT_FINAL, \n",
    "                                                      project_id=PROJECT_ID, \n",
    "                                                      dataname=DATANAME, \n",
    "                                                      table_id=TABLE_ID, \n",
    "                                                      location=LOCATION)\n",
    "print(\"BigQuery Upload completed. Status:\", bigquery_upload_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f8a4c-53ff-48c0-aea3-a11748c0a81c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
