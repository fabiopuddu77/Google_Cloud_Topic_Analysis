{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9893f668-bd43-429a-959c-6b346162156b",
   "metadata": {},
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a083580-a290-42a6-9b5a-442d1a24d696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from main import main\n",
    "import json\n",
    "from google.cloud import aiplatform as aip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce88051-d9f4-49b8-86b0-0d0a8be3b020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f95a1e-a782-4bbb-bea4-58ddf90d075d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Check if the file in exists in gcs output_processing.parquet\n",
      "\n",
      "INFO: output_processing.parquet  already exists in GCS. Skipping process.\n",
      "\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO: Check if the file in exists in gcs output_tokenized.parquet\n",
      "\n",
      "INFO: output_tokenized.parquet  already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Check if the file in exists in gcs output_sentiment.parquet\n",
      "\n",
      "INFO: File already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Check if the file in exists in gcs output_moderate.parquet\n",
      "\n",
      "INFO: File already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Check if the file in exists in gcs output_entities.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the file in exists in gcs\n",
      "output_processing.parquet already exists in GCS. Skipping process.\n",
      "Data Preprocessing completed. Processed data saved at: output_processing.parquet\n",
      "Check if the file in exists in gcs\n",
      "output_tokenized.parquet already exists in GCS. Skipping process.\n",
      "Tokenization completed. Tokenized data saved at: output_tokenized.parquet\n",
      "Check if the file in exists in gcs\n",
      "File already exists in GCS. Skipping process.\n",
      "Sentiment Analysis completed. Sentiment data saved at: output_sentiment.parquet\n",
      "Check if the file in exists in gcs\n",
      "File already exists in GCS. Skipping process.\n",
      "Text Moderation completed. Moderated data saved at: output_moderate.parquet\n",
      "Check if the file in exists in gcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: File already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Start loading DataFrame from output_entities.parquet\n",
      "\n",
      "INFO: Loaded DataFrame from output_entities.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in GCS. Skipping process.\n",
      "Entity Analysis completed. Entity data saved at: output_entities.parquet\n",
      "Start loading DataFrame from output_entities.parquet\n",
      "Loaded DataFrame from output_entities.parquet\n",
      "Loaded 20 rows into BigQuery table: gcp-ccai-auto-ml-contactcenter.datasetnlp.testdatabq\n",
      "BigQuery Upload completed. Status: testdatabq\n",
      "Create Dataframe prediction\n",
      "Adapt the variables to the autoML\n",
      "Create instances\n"
     ]
    }
   ],
   "source": [
    "\n",
    "newobs, instances, NOTEBOOK = main()\n",
    "aip.Endpoint.list(filter=f'labels.notebook={NOTEBOOK}')\n",
    "endpoint = aip.Endpoint.list(filter=f'labels.notebook={NOTEBOOK}')[0]\n",
    "REGION=\"europe-west3\"\n",
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": [newobs[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437e6d92-0af0-4f1a-b131-85f11f351499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Couldn't read data from file \n",
      "Warning: \"/home/jupyter/End-to-end-AutoML-Topic-Classification/Get\", this \n",
      "Warning: makes an empty POST.\n",
      "curl: (6) Could not resolve host: Predictions\n",
      "curl: (6) Could not resolve host: API\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": 400,\n",
      "    \"message\": \"Empty instances.\",\n",
      "    \"status\": \"INVALID_ARGUMENT\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981baed-def7-4ac3-94b0-eb7ff5207af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
