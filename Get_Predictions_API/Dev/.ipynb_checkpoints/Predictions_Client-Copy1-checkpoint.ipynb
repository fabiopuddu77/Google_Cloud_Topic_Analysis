{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d78a843-c560-4b81-9b36-44507c424de9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c3f63-d509-4cea-bdf8-a3508593b9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from main import main\n",
    "from google.cloud import aiplatform as aip\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48dbe30-88d6-4ea5-9603-16054fd368ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9857a478-6713-4c2e-a910-978ae692c3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Check if the file in exists in gcs output_processing.parquet\n",
      "\n",
      "INFO: output_processing.parquet  already exists in GCS. Skipping process.\n",
      "\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO: Check if the file in exists in gcs output_tokenized.parquet\n",
      "\n",
      "INFO: output_tokenized.parquet  already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Check if the file in exists in gcs output_sentiment.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the file in exists in gcs\n",
      "output_processing.parquet already exists in GCS. Skipping process.\n",
      "Data Preprocessing completed. Processed data saved at: output_processing.parquet\n",
      "Check if the file in exists in gcs\n",
      "output_tokenized.parquet already exists in GCS. Skipping process.\n",
      "Tokenization completed. Tokenized data saved at: output_tokenized.parquet\n",
      "Check if the file in exists in gcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: File already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Check if the file in exists in gcs output_moderate.parquet\n",
      "\n",
      "INFO: File already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Check if the file in exists in gcs output_entities.parquet\n",
      "\n",
      "INFO: File already exists in GCS. Skipping process.\n",
      "\n",
      "INFO: Start loading DataFrame from output_entities.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists in GCS. Skipping process.\n",
      "Sentiment Analysis completed. Sentiment data saved at: output_sentiment.parquet\n",
      "Check if the file in exists in gcs\n",
      "File already exists in GCS. Skipping process.\n",
      "Text Moderation completed. Moderated data saved at: output_moderate.parquet\n",
      "Check if the file in exists in gcs\n",
      "File already exists in GCS. Skipping process.\n",
      "Entity Analysis completed. Entity data saved at: output_entities.parquet\n",
      "Start loading DataFrame from output_entities.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded DataFrame from output_entities.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from output_entities.parquet\n",
      "Loaded 20 rows into BigQuery table: gcp-ccai-auto-ml-contactcenter.datasetnlp.testdatabq\n",
      "BigQuery Upload completed. Status: testdatabq\n",
      "Create Dataframe prediction\n",
      "Adapt the variables to the autoML\n",
      "Create instances\n",
      "ENDPOINTS: \n",
      " automl_datasetnlp_20240325224945\n",
      "PREDICTIONS \n",
      "\n",
      "+------------------------+------------+\n",
      "| Class                  |      Score |\n",
      "+========================+============+\n",
      "| Technology             | 0.00651041 |\n",
      "+------------------------+------------+\n",
      "| Sports                 | 0.928073   |\n",
      "+------------------------+------------+\n",
      "| Science                | 0.00101889 |\n",
      "+------------------------+------------+\n",
      "| Politics               | 0.0162563  |\n",
      "+------------------------+------------+\n",
      "| Health                 | 0.00204835 |\n",
      "+------------------------+------------+\n",
      "| Environment            | 0.00269876 |\n",
      "+------------------------+------------+\n",
      "| Business               | 0.0195512  |\n",
      "+------------------------+------------+\n",
      "| Arts and Entertainment | 0.0238427  |\n",
      "+------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    newobs, instances, NOTEBOOK = main()\n",
    "    aip.Endpoint.list(filter=f'labels.notebook={NOTEBOOK}')\n",
    "    endpoint = aip.Endpoint.list(filter=f'labels.notebook={NOTEBOOK}')[0]\n",
    "    print(f\"ENDPOINTS: \\n {endpoint.display_name}\")\n",
    "    prediction = endpoint.predict(instances = instances) # or instances = newobs\n",
    "    \n",
    "    dictionary=prediction.predictions[2]\n",
    "    # Convert the dictionary into a list of tuples\n",
    "    table_data = list(zip(dictionary['classes'], dictionary['scores']))\n",
    "\n",
    "    # Print the table\n",
    "    print(\"PREDICTIONS \\n\")\n",
    "    print(tabulate(table_data, headers=['Class', 'Score'], tablefmt='grid'))\n",
    "    #print(f\"PREDICTIONS: \\n {prediction.predictions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37476323-51dd-4efa-8885-d50c75d98040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
