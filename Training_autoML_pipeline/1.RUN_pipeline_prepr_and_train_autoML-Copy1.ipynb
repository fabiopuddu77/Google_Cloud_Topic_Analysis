{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062cfb0b-b3fc-4aca-a129-30e5e0f460cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.41.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.46.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.14.0)\n",
      "Collecting google-cloud-storage\n",
      "  Using cached google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /opt/conda/lib/python3.10/site-packages (2.11.0)\n",
      "Collecting google-cloud-pipeline-components\n",
      "  Downloading google_cloud_pipeline_components-2.13.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.17.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.11.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.5.3)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.15)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.7)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (26.1.0)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.18)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components) (3.1.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (2.1.3)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2023.11.17)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (69.0.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n",
      "Downloading google_cloud_aiplatform-1.46.0-py2.py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "Downloading google_cloud_pipeline_components-2.13.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Installing collected packages: protobuf, google-api-core, google-cloud-storage, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.1\n",
      "    Uninstalling google-api-core-1.34.1:\n",
      "      Successfully uninstalled google-api-core-1.34.1\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.14.0\n",
      "    Uninstalling google-cloud-storage-2.14.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.41.0\n",
      "    Uninstalling google-cloud-aiplatform-1.41.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.41.0\n",
      "  Attempting uninstall: google-cloud-pipeline-components\n",
      "    Found existing installation: google-cloud-pipeline-components 2.11.0\n",
      "    Uninstalling google-cloud-pipeline-components-2.11.0:\n",
      "      Successfully uninstalled google-cloud-pipeline-components-2.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.18.0 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.3 which is incompatible.\n",
      "google-cloud-translate 2.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.15.0, but you have google-api-core 2.18.0 which is incompatible.\n",
      "google-cloud-translate 2.0.1 requires google-cloud-core<2.0dev,>=1.1.0, but you have google-cloud-core 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.18.0 google-cloud-aiplatform-1.46.0 google-cloud-pipeline-components-2.13.0 google-cloud-storage-2.16.0 protobuf-4.25.3\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform  \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "01280da6-f62d-414a-8b81-ab5c94090ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "#from kfp.components import create_component_from_func\n",
    "from google.cloud import aiplatform as aip\n",
    "from typing import NamedTuple\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#import kfp.v2.dsl as dsl\n",
    "#import google_cloud_pipeline_components as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f9b6a3b3-5b76-4112-ae08-0558cc6f06b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "REGION= \"europe-west3\"\n",
    "REPO_NAME = \"repo-demo3\"\n",
    "SERVICE_ACCOUNT = \"944308723981-compute@developer.gserviceaccount.com\"\n",
    "BUCKET = \"ccai-storage\"\n",
    "PIPELINE_NAME = \"automl_pipeline\"\n",
    "YAML_NAME = f\"{PIPELINE_NAME}.yml\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline_root/\"\n",
    "DISPLAY_NAME = PIPELINE_NAME.replace(\"_\", \"-\")\n",
    "NOTEBOOK = \"automl\"\n",
    "DATANAME = \"datasetnlp\"\n",
    "FOLDER = 'pipeline'\n",
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "TABLE_ID = \"stepfinalbq\"\n",
    "TEXT_COLUMN = 'body_pre'\n",
    "LOCATION = \"europe-west3\"\n",
    "\n",
    "#BQ_SOURCE = \"bq://gcp-ccai-auto-ml-contactcenter.datasetnlp.stepfinalbq\"\n",
    "OUTPUT_PROCESSING = 'step1_pipeline.parquet'\n",
    "OUTPUT_TOKENIZATION = 'step2_pipeline.parquet'\n",
    "OUTPUT_SENTIMENT = 'step3_pipeline.parquet'\n",
    "OUTPUT_MODERATE = 'step4_pipeline.parquet'\n",
    "OUTPUT_ENTITIES = 'step5_pipeline.parquet'\n",
    "OUTPUT_FINAL = 'step_final_bq.parquet'\n",
    "ENDPOINT_NAME = 'automl_datasetnlp_endpoint1'\n",
    "\n",
    "FILE_PATH = 'articlesoutput.parquet'\n",
    "NUM_DOC = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f70cced8-cbcc-42ad-8b50-8bc352d4d0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "59e5ee76-b6ba-4e3f-a9a3-e3319de66388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resources\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'topic'\n",
    "VAR_OMIT = (\n",
    "    'uri,url,date,body,time,dateTime,dateTimePub,lang,isDuplicate,dataType,sentiment,' +\n",
    "    'eventUri,image,sharesFacebook,' +\n",
    "    'sourceLocationLabel,categoryLabels,' +\n",
    "    'categoryWeights,' +\n",
    "    'alexaCountryRank,date_column,year,year_month,' +\n",
    "    'num_documents,' +\n",
    "    'PERSON,OTHER,ORGANIZATION,' +\n",
    "    'EVENT,LOCATION,WORK_OF_ART,CONSUMER_GOOD,NUMBER,DATE,' +\n",
    "    'NUMBER_mean_salience,' +\n",
    "    'DATE_mean_salience,PRICE,ADDRESS,' +\n",
    "    'ADDRESS_mean_salience,PHONE_NUMBER,PHONE_NUMBER_mean_salience'\n",
    ")\n",
    "\n",
    "COLUMN_TOPICK = \"shares_scaled, body_pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "213338bf-6ae5-4244-a094-854f808b0e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uri,url,date,body,time,dateTime,dateTimePub,lang,isDuplicate,dataType,sentiment,eventUri,image,sharesFacebook,sourceLocationLabel,categoryLabels,categoryWeights,alexaCountryRank,date_column,year,year_month,num_documents,PERSON,OTHER,ORGANIZATION,EVENT,LOCATION,WORK_OF_ART,CONSUMER_GOOD,NUMBER,DATE,NUMBER_mean_salience,DATE_mean_salience,PRICE,ADDRESS,ADDRESS_mean_salience,PHONE_NUMBER,PHONE_NUMBER_mean_salience'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAR_OMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e73bf63c-38d2-40a0-ad1a-74a8bcb8aa80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'944308723981-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "591eb0ee-6b1d-4a54-99c0-223657c3757e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "25c33769-fd54-4964-ae3c-e9bff94ce38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52bca0b1-d93b-45c7-aa20-f7938478707e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=PIPELINE_ROOT, location=REGION)\n",
    "bq = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "516524a0-19ca-4ce7-9104-c256b619cb23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE\n",
      "organizations/329273198709/roles/Ruolopersonalizzato\n",
      "organizations/329273198709/roles/Ruolopersonalizzato517\n",
      "roles/aiplatform.user\n",
      "roles/artifactregistry.admin\n",
      "roles/artifactregistry.createOnPushWriter\n",
      "roles/artifactregistry.writer\n",
      "roles/bigquery.admin\n",
      "roles/cloudbuild.builds.builder\n",
      "roles/cloudtranslate.editor\n",
      "roles/contactcenterinsights.editor\n",
      "roles/containerregistry.ServiceAgent\n",
      "roles/dataflow.admin\n",
      "roles/dataflow.developer\n",
      "roles/dataflow.worker\n",
      "roles/datastore.user\n",
      "roles/dialogflow.client\n",
      "roles/pubsub.editor\n",
      "roles/run.developer\n",
      "roles/securesourcemanager.repoCreator\n",
      "roles/securesourcemanager.repoWriter\n",
      "roles/source.writer\n",
      "roles/storage.objectAdmin\n",
      "roles/viewer\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "11b31627-5cba-481a-a9fa-3b80540001b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp/automl'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}\n",
    "DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b0b1fe0-e96d-4764-8333-efd83bb31989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {URI}\n",
    "!mkdir -p {URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dc19a848-23ff-40f1-bc29-e2f2cba26508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ccai-storage/datasetnlp/models/automl'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08f918-2cef-4ac6-9258-5a1bce6eb863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c9dc312e-a723-40aa-a178-093cd7810bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creazione componenti custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a500dd6-cef4-4928-b313-ff81f73b8afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_preparation:latest\")\n",
    "\n",
    "def data_preprocessing(\n",
    "    bucket: str,\n",
    "    file_path: str,\n",
    "    folder: str,\n",
    "    parquet_file_name: str, \n",
    "    processed_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    \n",
    "    from processing.data_preparation import GCSParquetLoader\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = GCSParquetLoader(bucket, file_path, folder, parquet_file_name)\n",
    "    processed_dataset.uri = processor.process()\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "980b7ec0-3856-4952-ab86-e16d6b933f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_tokenization:latest\")\n",
    "\n",
    "def data_tokenization(\n",
    "    bucket: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    tokenized_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.tokenization import TokenizationProcessor\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = TokenizationProcessor(bucket, file_path.uri, folder, parquet_file_name)\n",
    "    #processor.save_df_to_gcs_parquet()\n",
    "    \n",
    "    print(\"--\")\n",
    "    tokenized_dataset.uri = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b013b467-b68b-456b-a2a7-335d44ea3f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_sentiment:latest\")\n",
    "\n",
    "def data_sentiment(\n",
    "    bucket: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    text_column: str,\n",
    "    num_doc: int,\n",
    "    sentiment_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.sentiment import GCSSentimentAnalyzer\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = GCSSentimentAnalyzer(bucket, file_path.uri, folder, parquet_file_name, \n",
    "                                    text_column, num_doc)\n",
    "    \n",
    "    \n",
    "    print(\"--\")\n",
    "    sentiment_dataset.uri = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d45ebcd-b826-4f99-a3d6-2f5856882883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_moderate:latest\")\n",
    "\n",
    "def data_moderate(\n",
    "    bucket: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    text_column: str,\n",
    "    num_doc: int,\n",
    "    moderate_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.moderate import GCSTextModerationLoader\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = GCSTextModerationLoader(bucket, file_path.uri, folder, parquet_file_name, \n",
    "                                    text_column, num_doc)\n",
    "    \n",
    "    print(\"--\")\n",
    "    moderate_dataset.uri = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1a1577d3-4e0a-4207-af7e-26848d308b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_entities:latest\")\n",
    "\n",
    "def data_entities(\n",
    "    bucket: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    text_column: str,\n",
    "    num_doc: int,\n",
    "    entities_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.entities import GCSCEntityAnalyzer\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    print(f\"------ file_path.uri = {file_path.uri}\")\n",
    "    processor = GCSCEntityAnalyzer(bucket, file_path.uri, folder, parquet_file_name, \n",
    "                                    text_column, num_doc)\n",
    "    \n",
    "    \n",
    "    entities_dataset.uri = processor.process()\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f64cadd9-386a-4504-a16d-a3125f5430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH=\"step5_pipeline.parquet\"\n",
    "\n",
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_bigquery:latest\")\n",
    "\n",
    "def data_bigquery(\n",
    "    bucket: str,\n",
    "    #file_path: Input[Artifact],\n",
    "    file_path: str,\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    project_id: str,\n",
    "    dataname: str,\n",
    "    table_id: str,\n",
    "    location: str,\n",
    "    big_query_gcs: Output[Artifact]\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"big_query_out\", str)  # Return parameters\n",
    "    ],\n",
    "    \n",
    "):  \n",
    "    \n",
    "    import logging\n",
    "    from processing.bigquery import GCS_Bigquery\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    #print(f\"------ file_path.uri = {file_path.uri}\")\n",
    "    print(\"--\")\n",
    "    processor = GCS_Bigquery(bucket, file_path, folder, parquet_file_name, \n",
    "                         project_id, dataname, table_id, location)\n",
    "    \n",
    "    #processor = GCS_Bigquery(bucket, file_path.uri, folder, parquet_file_name, \n",
    "    #                     project_id, dataname, table_id, location)\n",
    "    \n",
    "    \n",
    "    bigquery_table_out = processor.upload_dataframe_to_bigquery()\n",
    "    big_query_gcs.uri = f\"bq://{project_id}.{dataname}.{table_id}\"\n",
    "    return (big_query_gcs.uri,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "daca8348-c1b8-4aa5-a54a-43e7a10aecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "OPTIMIZATION_PREDICTION_TYPE = \"classification\"\n",
    "OPTIMIZATION_OBJECTIVE=\"minimize-log-loss\"\n",
    "BUDGET_MILLI_NODE_HOURS=1000\n",
    "DISABLE_EARLY_STOPPING=False\n",
    "PREDEFINED_SPLIT_COLUMN_NAME='split'\n",
    "THRESHOLDS_DICT_STR = '0.95'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9ac9ff15-05a0-4f79-bab4-b9767ef58521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "\n",
    "def classification_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Artifact],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform as aip\n",
    "\n",
    "    aip.init(project=PROJECT_ID, staging_bucket=PIPELINE_ROOT, location=REGION)\n",
    "    \n",
    "\n",
    "    def retrieve_existing_model_metrics() -> int:\n",
    "        \"\"\"\n",
    "        Retrieve the auRoc_existing metric from the evaluation of the existing model.\n",
    "        \"\"\"\n",
    "        # Retrieve the existing model and its evaluation\n",
    "        models = aip.Model.list(filter=f'labels.notebook={NOTEBOOK}')\n",
    "        model = models[1]\n",
    "        evaluation = model.get_model_evaluation().to_dict()\n",
    "\n",
    "        # Extract the log loss metric\n",
    "        auRoc_existing = evaluation['metrics']['auRoc']\n",
    "\n",
    "        return auRoc_existing\n",
    "\n",
    "\n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(model):\n",
    "        response = model.list_model_evaluations()\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            evaluation = evaluation.to_dict()\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation[\"name\"])\n",
    "            print(\" metrics_schema_uri:\", evaluation[\"metricsSchemaUri\"])\n",
    "            metrics = evaluation[\"metrics\"]\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation[\"name\"],\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # Use the given metrics threshold(s) to determine whether the model is\n",
    "    # accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v and metrics_dict[k] < retrieve_existing_model_metrics():   # if under threshold, and under previous model aurRoc don't deploy\n",
    "                    logging.info(\"{} < {}; returning False\".format(metrics_dict[k], v))\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.metadata[\"resourceName\"]\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    # Get the trained model resource\n",
    "    model = aip.Model(model_resource_path)\n",
    "\n",
    "    # Get model evaluation metrics from the the trained model\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(model)\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    classification_model_eval_metrics, \"tabular_eval_component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ca6f818-4f0f-4fc8-9573-24aa278bee3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name=f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}',\n",
    "    pipeline_root=URI+'/'+str(TIMESTAMP)+'/kfp/',\n",
    "    description=\"Data preprocessing text and Training Automl topic classification\"\n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    dataname: str,\n",
    "    display_name: str,\n",
    "    endpoint_name: str,\n",
    "    deploy_machine: str,\n",
    "    var_target: str,\n",
    "    var_omit: str,\n",
    "    features: dict,\n",
    "    labels: dict,\n",
    "    bucket: str,  # Added type annotation\n",
    "    file_path: str,  # Added type annotation\n",
    "    folder: str,  # Added type annotation\n",
    "    project_id: str,  # Added type annotation\n",
    "    table_id: str,  # Added type annotation\n",
    "    text_column: str,  # Added type annotation\n",
    "    location: str,  # Added type annotation\n",
    "    num_doc: int,  # Added type annotation\n",
    "    output_processing: str,  # Added type annotation\n",
    "    output_tokenization: str,  # Added type annotation\n",
    "    output_sentiment: str,  # Added type annotation\n",
    "    output_moderate: str,  # Added type annotation\n",
    "    output_entities: str,  # Added type annotation\n",
    "    output_final: str, # Added type annotation\n",
    "    optimization_prediction_type: str,\n",
    "    optimization_objective: str,\n",
    "    budget_milli_node_hours: int,\n",
    "    disable_early_stopping: bool,\n",
    "    predefined_split_column_name: str,\n",
    "    thresholds_dict_str : str\n",
    "\n",
    "):\n",
    "\n",
    "    \n",
    "    bigquery_op = data_bigquery(\n",
    "        bucket=bucket,\n",
    "        #file_path=entities_op.outputs['entities_dataset'],\n",
    "        file_path=file_path,\n",
    "        folder=folder,\n",
    "        parquet_file_name=output_final,\n",
    "        project_id=project_id,\n",
    "        dataname=dataname,\n",
    "        table_id=table_id,\n",
    "        location=location\n",
    "    )\n",
    "        \n",
    "    # dataset\n",
    "    dataset_create_op  = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        bq_source=bigquery_op.outputs[\"big_query_out\"],\n",
    "        labels=labels,\n",
    "        location=location\n",
    "    )\n",
    "    \n",
    "    training_op = AutoMLTabularTrainingJobRunOp(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        optimization_prediction_type = optimization_prediction_type,\n",
    "        optimization_objective=optimization_objective,\n",
    "        budget_milli_node_hours = budget_milli_node_hours,\n",
    "        disable_early_stopping=disable_early_stopping,\n",
    "        column_specs = features,\n",
    "        dataset = dataset_create_op.outputs['dataset'],\n",
    "        target_column = var_target,\n",
    "        predefined_split_column_name = 'split',\n",
    "        labels = labels,\n",
    "        location = location\n",
    "    )\n",
    "    \n",
    "    model_eval_task = classification_model_eval_metrics(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        thresholds_dict_str=thresholds_dict_str,\n",
    "        model=training_op.outputs[\"model\"],\n",
    "    )\n",
    "    \n",
    "    with dsl.If(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        endpoint_op = EndpointCreateOp(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            labels = labels,\n",
    "            display_name=endpoint_name,\n",
    "        )\n",
    "        # Endpoint: Deployment of Model\n",
    "        deployment = ModelDeployOp(\n",
    "            model = training_op.outputs[\"model\"],\n",
    "            endpoint = endpoint_op.outputs[\"endpoint\"],\n",
    "            dedicated_resources_min_replica_count = 1,\n",
    "            dedicated_resources_max_replica_count = 1,\n",
    "            traffic_split = {\"0\": 100},\n",
    "            dedicated_resources_machine_type= deploy_machine\n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "440ebe04-3855-4e8e-901b-918f56ab0107",
   "metadata": {},
   "source": [
    "@component\n",
    "def auto_ml(\n",
    "        project: str,\n",
    "        display_name: str,\n",
    "        optimization_prediction_type: str,\n",
    "        optimization_objective: str,\n",
    "        budget_milli_node_hours: int,\n",
    "        disable_early_stopping: bool,\n",
    "        column_specs: dict,\n",
    "        dataset: Input[Artifact],\n",
    "        target_column: str,\n",
    "        predefined_split_column_name: str,\n",
    "        labels: dict,\n",
    "        location: str,\n",
    "        model_output: Output[Artifact]\n",
    "        ):\n",
    "    \n",
    "        from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "        model = AutoMLTabularTrainingJobRunOp(\n",
    "                project = project,\n",
    "                display_name = display_name,\n",
    "                optimization_prediction_type =optimization_prediction_type,\n",
    "                optimization_objective=optimization_objective,\n",
    "                budget_milli_node_hours = budget_milli_node_hours,\n",
    "                disable_early_stopping=disable_early_stopping,\n",
    "                column_specs = features,\n",
    "                dataset = dataset.outputs['dataset'],\n",
    "                target_column = var_target,\n",
    "                predefined_split_column_name = predefined_split_column_name,\n",
    "                labels = labels,\n",
    "                location = REGION)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b4ada-4963-4256-be98-0b7edc0d9cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc7538-a8f6-47ad-8a7e-4accc372bd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b1e41684-54a6-4469-ba65-b8aaab1a3db4",
   "metadata": {
    "tags": []
   },
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}',\n",
    "    pipeline_root=URI+'/'+str(TIMESTAMP)+'/kfp/',\n",
    "    description=\"Data preprocessing text and Training Automl topic classification\"\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    dataname: str,\n",
    "    display_name: str,\n",
    "    endpoint_name: str,\n",
    "    deploy_machine: str,\n",
    "    var_target: str,\n",
    "    var_omit: str,\n",
    "    features: dict,\n",
    "    labels: dict,\n",
    "    bucket: str,  # Added type annotation\n",
    "    file_path: str,  # Added type annotation\n",
    "    folder: str,  # Added type annotation\n",
    "    project_id: str,  # Added type annotation\n",
    "    table_id: str,  # Added type annotation\n",
    "    text_column: str,  # Added type annotation\n",
    "    location: str,  # Added type annotation\n",
    "    num_doc: int,  # Added type annotation\n",
    "    output_processing: str,  # Added type annotation\n",
    "    output_tokenization: str,  # Added type annotation\n",
    "    output_sentiment: str,  # Added type annotation\n",
    "    output_moderate: str,  # Added type annotation\n",
    "    output_entities: str,  # Added type annotation\n",
    "    output_final: str  # Added type annotation\n",
    "    optimization_prediction_type: str,\n",
    "    optimization_objective: str,\n",
    "    budget_milli_node_hours: int,\n",
    "    disable_early_stopping: bool,\n",
    "    column_specs: dict,\n",
    "    target_column: str,\n",
    "    predefined_split_column_name: str,\n",
    "    labels: dict,\n",
    "):\n",
    " \"\"\"   \n",
    "    processing_op = data_preprocessing(\n",
    "        bucket=bucket, \n",
    "        file_path=file_path, \n",
    "        folder=folder, \n",
    "        parquet_file_name=output_processing\n",
    "    )\n",
    "    \n",
    "    tokenization_op = data_tokenization(\n",
    "        bucket=bucket, \n",
    "        file_path=processing_op.outputs[\"processed_dataset\"], \n",
    "        folder=folder, \n",
    "        parquet_file_name=output_tokenization\n",
    "    )\n",
    "    \n",
    "    sentiment_op = data_sentiment(\n",
    "        bucket=bucket, \n",
    "        file_path=tokenization_op.outputs[\"tokenized_dataset\"], \n",
    "        folder=folder, \n",
    "        parquet_file_name=output_sentiment,\n",
    "        text_column=text_column, \n",
    "        num_doc=num_doc\n",
    "    )\n",
    "    \n",
    "    moderate_op = data_moderate(\n",
    "        bucket=bucket, \n",
    "        file_path=sentiment_op.outputs[\"sentiment_dataset\"], \n",
    "        folder=folder, \n",
    "        parquet_file_name=output_moderate,\n",
    "        text_column=text_column, \n",
    "        num_doc=num_doc\n",
    "    )\n",
    "        \n",
    "    entities_op = data_entities(\n",
    "        bucket=bucket, \n",
    "        file_path=moderate_op.outputs[\"moderate_dataset\"], \n",
    "        folder=folder, \n",
    "        parquet_file_name=output_entities,\n",
    "        text_column=text_column, \n",
    "        num_doc=num_doc\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    bigquery_op = data_bigquery(\n",
    "        bucket=bucket,\n",
    "        #file_path=entities_op.outputs['entities_dataset'],\n",
    "        file_path=file_path,\n",
    "        folder=folder,\n",
    "        parquet_file_name=output_final,\n",
    "        project_id=project_id,\n",
    "        dataname=dataname,\n",
    "        table_id=table_id,\n",
    "        location=location\n",
    "    )\n",
    "        \n",
    "    # dataset\n",
    "    dataset = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        bq_source=bigquery_op.outputs[\"big_query_out\"],\n",
    "        labels=labels,\n",
    "        location=location\n",
    "    )\n",
    "    \n",
    "\n",
    "    # training\n",
    "    auto_ml_op = auto_ml(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        optimization_prediction_type = optimization_prediction_type,\n",
    "        optimization_objective = optimization_objective,\n",
    "        budget_milli_node_hours = budget_milli_node_hours,\n",
    "        disable_early_stopping=disable_early_stopping,\n",
    "        column_specs = features,\n",
    "        dataset = dataset.outputs['dataset'],\n",
    "        target_column = var_target,\n",
    "        predefined_split_column_name = predefined_split_column_name,\n",
    "        labels = labels,\n",
    "        location = location\n",
    "  \n",
    "    \"\"\"\n",
    "    model = AutoMLTabularTrainingJobRunOp(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        optimization_prediction_type = \"classification\",\n",
    "        optimization_objective=\"minimize-log-loss\",\n",
    "        budget_milli_node_hours = 1000,\n",
    "        disable_early_stopping=False,\n",
    "        column_specs = features,\n",
    "        dataset = dataset.outputs['dataset'],\n",
    "        target_column = var_target,\n",
    "        predefined_split_column_name = 'split',\n",
    "        labels = labels,\n",
    "        location = REGION\n",
    "    )\n",
    "    \n",
    "        \n",
    "    # Endpoint: Creation\n",
    "    endpoint = EndpointCreateOp(\n",
    "        project = project,\n",
    "        display_name = endpoint_name,\n",
    "        labels = labels,\n",
    "        location = REGION\n",
    "    )\n",
    "    \n",
    "    # Endpoint: Deployment of Model\n",
    "    deployment = ModelDeployOp(\n",
    "        model = model.outputs[\"model\"],\n",
    "        endpoint = endpoint.outputs[\"endpoint\"],\n",
    "        dedicated_resources_min_replica_count = 1,\n",
    "        dedicated_resources_max_replica_count = 1,\n",
    "        traffic_split = {\"0\": 100},\n",
    "        dedicated_resources_machine_type= deploy_machine\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "6eea2c0e-b0bd-4b6c-aa4a-df06a685c829",
   "metadata": {
    "tags": []
   },
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}',\n",
    "    pipeline_root=URI+'/'+str(TIMESTAMP)+'/kfp/',\n",
    "    description=\"Data preprocessing text and Training Automl topic classification\"\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    dataname: str,\n",
    "    display_name: str,\n",
    "    endpoint_name: str,\n",
    "    deploy_machine: str,\n",
    "    var_target: str,\n",
    "    var_omit: str,\n",
    "    features: dict,\n",
    "    labels: dict,\n",
    "    bucket: str,  # Added type annotation\n",
    "    file_path: str,  # Added type annotation\n",
    "    folder: str,  # Added type annotation\n",
    "    project_id: str,  # Added type annotation\n",
    "    table_id: str,  # Added type annotation\n",
    "    text_column: str,  # Added type annotation\n",
    "    location: str,  # Added type annotation\n",
    "    num_doc: int,  # Added type annotation\n",
    "    output_processing: str,  # Added type annotation\n",
    "    output_tokenization: str,  # Added type annotation\n",
    "    output_sentiment: str,  # Added type annotation\n",
    "    output_moderate: str,  # Added type annotation\n",
    "    output_entities: str,  # Added type annotation\n",
    "    output_final: str,  # Added type annotation\n",
    "    optimization_prediction_type: str,\n",
    "    optimization_objective: str,\n",
    "    budget_milli_node_hours: int,\n",
    "    disable_early_stopping: bool,\n",
    "    predefined_split_column_name: str,\n",
    "):\n",
    "\n",
    "\n",
    "    bigquery_op = data_bigquery(\n",
    "        bucket=bucket,\n",
    "        file_path=file_path,\n",
    "        folder=folder,\n",
    "        parquet_file_name=output_final,\n",
    "        project_id=project_id,\n",
    "        dataname=dataname,\n",
    "        table_id=table_id,\n",
    "        location=location\n",
    "    )\n",
    "        \n",
    "    # dataset\n",
    "    dataset = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        bq_source=bigquery_op.outputs[\"big_query_out\"],\n",
    "        labels=labels,\n",
    "        location=location\n",
    "    )\n",
    "    \n",
    "\n",
    "    # training\n",
    "    auto_ml_op = auto_ml(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        optimization_prediction_type = optimization_prediction_type,\n",
    "        optimization_objective = optimization_objective,\n",
    "        budget_milli_node_hours = budget_milli_node_hours,\n",
    "        disable_early_stopping=disable_early_stopping,\n",
    "        column_specs = features,\n",
    "        dataset = dataset.outputs['dataset'],\n",
    "        target_column = var_target,\n",
    "        predefined_split_column_name = predefined_split_column_name,\n",
    "        labels = labels,\n",
    "        location = location\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    model = AutoMLTabularTrainingJobRunOp(\n",
    "        project = project,\n",
    "        display_name = display_name,\n",
    "        optimization_prediction_type = \"classification\",\n",
    "        optimization_objective=\"minimize-log-loss\",\n",
    "        budget_milli_node_hours = 1000,\n",
    "        disable_early_stopping=False,\n",
    "        column_specs = features,\n",
    "        dataset = dataset.outputs['dataset'],\n",
    "        target_column = var_target,\n",
    "        predefined_split_column_name = 'split',\n",
    "        labels = labels,\n",
    "        location = REGION\n",
    "    )\n",
    "    \n",
    "        \n",
    "    # Endpoint: Creation\n",
    "    endpoint = EndpointCreateOp(\n",
    "        project = project,\n",
    "        display_name = endpoint_name,\n",
    "        labels = labels,\n",
    "        location = REGION\n",
    "    )\n",
    "    \n",
    "    # Endpoint: Deployment of Model\n",
    "    deployment = ModelDeployOp(\n",
    "        model = model.outputs[\"model\"],\n",
    "        endpoint = endpoint.outputs[\"endpoint\"],\n",
    "        dedicated_resources_min_replica_count = 1,\n",
    "        dedicated_resources_max_replica_count = 1,\n",
    "        traffic_split = {\"0\": 100},\n",
    "        dedicated_resources_machine_type= deploy_machine\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a1986add-fa6f-4bf8-8114-cfdad0e4f32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f\"{DIR}/{NOTEBOOK}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5d89dd4f-88ea-4d66-9c16-e271ed5ae913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://temp/automl/automl.json [Content-Type=application/json]...\n",
      "/ [1 files][ 64.9 KiB/ 64.9 KiB]                                                \n",
      "Operation completed over 1 objects/64.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {DIR}/{NOTEBOOK}.json {URI}/{TIMESTAMP}/kfp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "33d13169-c865-4182-b6e7-732cea29201a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get feature names\n",
    "query = f\"SELECT * FROM {DATANAME}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{TABLE_ID}'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "OMIT = VAR_OMIT.split(\",\") + [VAR_TARGET, 'split']\n",
    "features = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "features = dict.fromkeys(features, 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "802bd3b6-02ca-4851-a05f-371b40ba1bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uri', 'url', 'title', 'body', 'date', 'time', 'dateTime',\n",
       "       'dateTimePub', 'lang', 'isDuplicate', 'dataType', 'sentiment',\n",
       "       'eventUri', 'relevance', 'image', 'authors', 'sharesFacebook',\n",
       "       'sourceTitle', 'sourceLocationLabel', 'categoryLabels',\n",
       "       'categoryWeights', 'importanceRank', 'alexaGlobalRank',\n",
       "       'alexaCountryRank', 'date_column', 'year', 'month', 'year_month',\n",
       "       'topic', 'split', 'shares_scaled', 'body_pre', 'score', 'magnitude',\n",
       "       'num_documents', 'Toxic', 'Insult', 'Profanity', 'Derogatory', 'Sexual',\n",
       "       'Death_Harm__Tragedy', 'Violent', 'Firearms__Weapons', 'Public_Safety',\n",
       "       'Health', 'Religion__Belief', 'Illicit_Drugs', 'War__Conflict',\n",
       "       'Politics', 'Finance', 'Legal', 'PERSON', 'OTHER', 'ORGANIZATION',\n",
       "       'EVENT', 'LOCATION', 'WORK_OF_ART', 'CONSUMER_GOOD', 'NUMBER',\n",
       "       'PERSON_mean_salience', 'OTHER_mean_salience',\n",
       "       'ORGANIZATION_mean_salience', 'EVENT_mean_salience',\n",
       "       'LOCATION_mean_salience', 'WORK_OF_ART_mean_salience',\n",
       "       'CONSUMER_GOOD_mean_salience', 'NUMBER_mean_salience',\n",
       "       'DATE_mean_salience', 'PRICE', 'PRICE_mean_salience', 'ADDRESS',\n",
       "       'ADDRESS_mean_salience', 'PHONE_NUMBER', 'PHONE_NUMBER_mean_salience'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = f\"SELECT * FROM {DATANAME}.{TABLE_ID}\"\n",
    "df2 = bq.query(query2).to_dataframe()\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4623b9fd-0936-4571-9171-41f0a88fce38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'auto',\n",
       " 'relevance': 'auto',\n",
       " 'authors': 'auto',\n",
       " 'sourceTitle': 'auto',\n",
       " 'importanceRank': 'auto',\n",
       " 'alexaGlobalRank': 'auto',\n",
       " 'month': 'auto',\n",
       " 'shares_scaled': 'auto',\n",
       " 'body_pre': 'auto',\n",
       " 'score': 'auto',\n",
       " 'magnitude': 'auto',\n",
       " 'Toxic': 'auto',\n",
       " 'Insult': 'auto',\n",
       " 'Profanity': 'auto',\n",
       " 'Derogatory': 'auto',\n",
       " 'Sexual': 'auto',\n",
       " 'Death_Harm__Tragedy': 'auto',\n",
       " 'Violent': 'auto',\n",
       " 'Firearms__Weapons': 'auto',\n",
       " 'Public_Safety': 'auto',\n",
       " 'Health': 'auto',\n",
       " 'Religion__Belief': 'auto',\n",
       " 'Illicit_Drugs': 'auto',\n",
       " 'War__Conflict': 'auto',\n",
       " 'Politics': 'auto',\n",
       " 'Finance': 'auto',\n",
       " 'Legal': 'auto',\n",
       " 'PERSON_mean_salience': 'auto',\n",
       " 'OTHER_mean_salience': 'auto',\n",
       " 'ORGANIZATION_mean_salience': 'auto',\n",
       " 'EVENT_mean_salience': 'auto',\n",
       " 'LOCATION_mean_salience': 'auto',\n",
       " 'WORK_OF_ART_mean_salience': 'auto',\n",
       " 'CONSUMER_GOOD_mean_salience': 'auto',\n",
       " 'PRICE_mean_salience': 'auto'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9761bd61-0c1f-4939-a507-265778105e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = aip.PipelineJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    template_path = f\"{URI}/{TIMESTAMP}/kfp/{NOTEBOOK}.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values = {\n",
    "        \"project\" : PROJECT_ID,\n",
    "        \"dataname\" : DATANAME,\n",
    "        \"display_name\" : f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "        \"endpoint_name\" : ENDPOINT_NAME,\n",
    "        \"deploy_machine\" : DEPLOY_COMPUTE,\n",
    "        \"var_target\" : VAR_TARGET,\n",
    "        \"var_omit\" : VAR_OMIT,\n",
    "        \"thresholds_dict_str\": THRESHOLDS_DICT_STR,\n",
    "        \"optimization_prediction_type\": OPTIMIZATION_PREDICTION_TYPE,\n",
    "        \"optimization_objective\": OPTIMIZATION_OBJECTIVE,\n",
    "        \"budget_milli_node_hours\": BUDGET_MILLI_NODE_HOURS,\n",
    "        \"disable_early_stopping\": DISABLE_EARLY_STOPPING,\n",
    "        \"predefined_split_column_name\": PREDEFINED_SPLIT_COLUMN_NAME,\n",
    "        \"features\" : {'title': 'text',\n",
    "                     'relevance': 'numeric',\n",
    "                     'authors': 'categorical',\n",
    "                     'sourceTitle': 'categorical',\n",
    "                     'importanceRank': 'numeric',\n",
    "                     'alexaGlobalRank': 'numeric',\n",
    "                     'month': 'categorical',\n",
    "                     'shares_scaled': 'numeric',\n",
    "                     'body_pre': 'text',\n",
    "                     'score': 'numeric',\n",
    "                     'magnitude': 'numeric',\n",
    "                     'Toxic': 'numeric',\n",
    "                     'Insult': 'numeric',\n",
    "                     'Profanity': 'numeric',\n",
    "                     'Derogatory': 'numeric',\n",
    "                     'Sexual': 'numeric',\n",
    "                     'Death_Harm__Tragedy': 'numeric',\n",
    "                     'Violent': 'numeric',\n",
    "                     'Firearms__Weapons': 'numeric',\n",
    "                     'Public_Safety': 'numeric',\n",
    "                     'Health': 'numeric',\n",
    "                     'Religion__Belief': 'numeric',\n",
    "                     'Illicit_Drugs': 'numeric',\n",
    "                     'War__Conflict': 'numeric',\n",
    "                     'Politics': 'numeric',\n",
    "                     'Finance': 'numeric',\n",
    "                     'Legal': 'numeric',\n",
    "                     'PERSON_mean_salience': 'numeric',\n",
    "                     'OTHER_mean_salience': 'numeric',\n",
    "                     'ORGANIZATION_mean_salience': 'numeric',\n",
    "                     'EVENT_mean_salience': 'numeric',\n",
    "                     'LOCATION_mean_salience': 'numeric',\n",
    "                     'WORK_OF_ART_mean_salience': 'numeric',\n",
    "                     'CONSUMER_GOOD_mean_salience': 'numeric',\n",
    "                     'PRICE_mean_salience': 'numeric'},\n",
    "        \"labels\" : {'notebook': NOTEBOOK},\n",
    "        \"bucket\" : BUCKET,\n",
    "        \"file_path\": FILE_PATH,\n",
    "        \"folder\": FOLDER,\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"table_id\": TABLE_ID,\n",
    "        \"text_column\": TEXT_COLUMN,\n",
    "        \"location\": LOCATION,\n",
    "        \"num_doc\": NUM_DOC,\n",
    "        \"output_processing\": OUTPUT_PROCESSING,\n",
    "        \"output_tokenization\": OUTPUT_TOKENIZATION,\n",
    "        \"output_sentiment\": OUTPUT_SENTIMENT,\n",
    "        \"output_moderate\": OUTPUT_MODERATE,\n",
    "        \"output_entities\": OUTPUT_ENTITIES,\n",
    "        \"output_final\": OUTPUT_FINAL\n",
    "\n",
    "    },\n",
    "    labels = {'notebook': NOTEBOOK},\n",
    "    enable_caching=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "51c4b935-d8b1-42b1-a18f-687db2d9e050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/944308723981/locations/europe-west3/pipelineJobs/kfp-automl-datasetnlp-20240403102909-20240403112252\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/944308723981/locations/europe-west3/pipelineJobs/kfp-automl-datasetnlp-20240403102909-20240403112252')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west3/pipelines/runs/kfp-automl-datasetnlp-20240403102909-20240403112252?project=944308723981\n"
     ]
    }
   ],
   "source": [
    "pipeline.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254c386-def4-44c4-a219-f05db918affd",
   "metadata": {},
   "source": [
    "Visual Representation of the pipeline can be viewed in the colsole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2797f868-3f60-44d9-a027-f9f21277348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Pipeline as it runs here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west3/pipelines/runs/kfp-automl-datasetnlp-20240325224945-20240325225059?project=gcp-ccai-auto-ml-contactcenter\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review the Pipeline as it runs here:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/pipelines/runs/{pipeline.resource_name.split('/')[-1]}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dc3205f7-5fdb-49a6-abce-eed4825da979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name=\"automl_datasetnlp_20240324221421\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb9f67-479b-41d0-8193-ad1b32026ce7",
   "metadata": {},
   "source": [
    "Retrieve the pipeline information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d6164344-f1e6-48c5-b3fe-f5d49373ba7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints_aip = aip.Endpoint.list(filter=(\"display_name={}\").format(endpoint_name),\n",
    "                                 location=LOCATION)\n",
    "endpoints_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae81c5c-00e0-4a43-950a-9b20af8c5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = None if len(endpoints_aip)==0 else endpoints_aip[0]\n",
    "\n",
    "if not endpoint:\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "         display_name=endpoint_name,\n",
    "         project=project_id,\n",
    "         location=location\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb96cafe-4269-4ad1-b9e6-13df4b14eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py\", line 65, in error_remapped_callable\n",
      "    if not hasattr(callable_, \"__name__\"):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/grpc/_channel.py\", line 1160, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/grpc/_channel.py\", line 1003, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.NOT_FOUND\n",
      "\tdetails = \"Resource not found.; GetContext is unable to find context resource with name: projects/944308723981/locations/us-central1/metadataStores/default/contexts/kfp-automl-datasetnlp-20240403074448\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.186.138:443 {created_time:\"2024-04-03T07:45:24.768020842+00:00\", grpc_status:5, grpc_message:\"Resource not found.; GetContext is unable to find context resource with name: projects/944308723981/locations/us-central1/metadataStores/default/contexts/kfp-automl-datasetnlp-20240403074448\"}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/tmp/ipykernel_154903/1875271929.py\", line 1, in <module>\n",
      "    aip.get_pipeline_df(pipeline = f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}')\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/metadata/metadata.py\", line 102, in get_pipeline_df\n",
      "    pipeline_resource_name = (\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/metadata/metadata.py\", line 134, in _get_experiment_or_pipeline_resource_name\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/metadata/resource.py\", line 102, in __init__\n",
      "    self._gca_resource = getattr(self.api_client, self._getter_method)(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/metadata_service/client.py\", line 2239, in get_context\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
      "    timeout = TimeToDeadlineTimeout(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py\", line 191, in retry_target\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py\", line 67, in error_remapped_callable\n",
      "google.api_core.exceptions.NotFound: 404 Resource not found.; GetContext is unable to find context resource with name: projects/944308723981/locations/us-central1/metadataStores/default/contexts/kfp-automl-datasetnlp-20240403074448\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "aip.get_pipeline_df(pipeline = f'kfp-{NOTEBOOK}-{DATANAME}-{TIMESTAMP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694b70a-c90c-457e-ad55-3294f5399868",
   "metadata": {},
   "source": [
    "\n",
    "### EVALUATION\n",
    "Although the aforementioned model was trained utilizing AutoML via the API, users can still assess the evaluation metrics directly within the Google Cloud Console. By navigating to the Models section of the Vertex AI service and selecting the model, users will be presented with comprehensive evaluation metrics accompanied by various helpful visual aids.\n",
    "\n",
    "Additionally, users have the option to retrieve the evaluation metrics for their model using the API. The following section outlines the process of utilizing the API for this purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26965a2-12c9-4bae-98ce-8c844d358023",
   "metadata": {},
   "source": [
    "Get the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6a08e2-1e9b-4689-8d46-e3219e3a3658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bfff997-991c-4665-8686-d5e16bf1ed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.models.Model object at 0x7f538d297a30> \n",
       " resource name: projects/944308723981/locations/europe-west3/models/2095625182075944960,\n",
       " <google.cloud.aiplatform.models.Model object at 0x7f538d296c20> \n",
       " resource name: projects/944308723981/locations/europe-west3/models/8349999164586721280,\n",
       " <google.cloud.aiplatform.models.Model object at 0x7f538d296800> \n",
       " resource name: projects/944308723981/locations/europe-west3/models/7334437448614674432]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = aip.Model.list(filter=f'labels.notebook={NOTEBOOK}')\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c092adf-29ec-4429-ae2a-ebe83ceb99c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/944308723981/locations/europe-west3/models/2095625182075944960'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[0]\n",
    "model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "725be706-849e-478c-8fb8-e622046d5db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_eval_info(model):\n",
    "    response = model.list_model_evaluations()\n",
    "    metrics_list = []\n",
    "    metrics_string_list = []\n",
    "    for evaluation in response:\n",
    "        evaluation = evaluation.to_dict()\n",
    "        print(\"model_evaluation\")\n",
    "        print(\" name:\", evaluation[\"name\"])\n",
    "        print(\" metrics_schema_uri:\", evaluation[\"metricsSchemaUri\"])\n",
    "        metrics = evaluation[\"metrics\"]\n",
    "        #for metric in metrics.keys():\n",
    "            #print(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "        metrics_str = json.dumps(metrics)\n",
    "        metrics_list.append(metrics)\n",
    "        metrics_string_list.append(metrics_str)\n",
    "\n",
    "    return (\n",
    "        evaluation[\"name\"],\n",
    "        metrics_list,\n",
    "        metrics_string_list,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e72c3bea-9c01-4373-a33a-5930bd075c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_evaluation\n",
      " name: projects/944308723981/locations/europe-west3/models/2095625182075944960@1/evaluations/4587355914663221019\n",
      " metrics_schema_uri: gs://google-cloud-aiplatform/schema/modelevaluation/classification_metrics_1.0.0.yaml\n",
      "got evaluation name: %s projects/944308723981/locations/europe-west3/models/2095625182075944960@1/evaluations/4587355914663221019\n"
     ]
    }
   ],
   "source": [
    "eval_name, metrics_list, metrics_str_list = get_eval_info(model=model)\n",
    "print(\"got evaluation name: %s\", eval_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09276a6e-b3d5-4c1c-999a-ece18be0331c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'outputs'"
     ]
    }
   ],
   "source": [
    "model.outputs[\"log_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cae013-cc64-4bfb-80e3-0c5355961894",
   "metadata": {},
   "source": [
    "Retrives the aggregate model evalution metrics for the model as a whole.  \n",
    "\n",
    "Either:\n",
    "- First, use `model.list_model_evaluations()` to retrieve the evaluation id, then use `model.get_model_evaluation(evaluation_id = )` for the evaluation id\n",
    "- Or, use `.get_model_evaluation()` and it will retrieve the first model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dde9fa2c-3556-43cd-b6a4-a111da9dd4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-3110e1a6-ccdd-47b0-b35d-47b42f4ac113\" href=\"#view-view-vertex-resource-3110e1a6-ccdd-47b0-b35d-47b42f4ac113\">\n",
       "          <span class=\"material-icons view-vertex-icon\">model_training</span>\n",
       "          <span>View Model Evaluation</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-3110e1a6-ccdd-47b0-b35d-47b42f4ac113');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/models/locations/europe-west3/models/2095625182075944960/versions/1/evaluations/4587355914663221019?project=944308723981');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/models/locations/europe-west3/models/2095625182075944960/versions/1/evaluations/4587355914663221019?project=944308723981', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = model.get_model_evaluation().to_dict() # get first evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3449110-57e6-47aa-af66-328aacec7d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'metricsSchemaUri', 'metrics', 'createTime', 'sliceDimensions', 'modelExplanation'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8f623f2-ece9-41f6-8c27-65f6c445023a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['auPrc', 'auRoc', 'logLoss', 'confusionMatrix', 'confidenceMetrics'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['metrics'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1db8588-0371-42e0-87b0-32261c25fb86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95601124"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['metrics']['auRoc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4b6a687-0b3a-4f0e-81a9-86a70e815539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'falseNegativeCount': '1',\n",
       " 'recallAt1': 0.7526096,\n",
       " 'falsePositiveRateAt1': 0.035341486,\n",
       " 'truePositiveCount': '957',\n",
       " 'recall': 0.99895614,\n",
       " 'trueNegativeCount': '1723',\n",
       " 'confidenceThreshold': 0.005,\n",
       " 'falsePositiveRate': 0.7430659,\n",
       " 'precisionAt1': 0.7526096,\n",
       " 'confusionMatrix': {'rows': [[13.0, 1.0, 1.0, 3.0, 1.0, 0.0, 5.0, 7.0, 0.0],\n",
       "   [0.0, 76.0, 0.0, 0.0, 4.0, 0.0, 3.0, 8.0, 0.0],\n",
       "   [0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 4.0, 0.0],\n",
       "   [2.0, 1.0, 0.0, 247.0, 0.0, 4.0, 19.0, 16.0, 0.0],\n",
       "   [0.0, 2.0, 1.0, 1.0, 40.0, 0.0, 4.0, 8.0, 0.0],\n",
       "   [0.0, 0.0, 1.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 3.0, 0.0, 49.0, 0.0, 5.0, 101.0, 26.0, 0.0],\n",
       "   [0.0, 7.0, 0.0, 24.0, 8.0, 2.0, 14.0, 233.0, 0.0],\n",
       "   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       "  'annotationSpecs': [{'displayName': 'Technology', 'id': '0'},\n",
       "   {'displayName': 'Sports', 'id': '1'},\n",
       "   {'displayName': 'Science', 'id': '2'},\n",
       "   {'displayName': 'Politics', 'id': '3'},\n",
       "   {'displayName': 'Health', 'id': '4'},\n",
       "   {'displayName': 'Environment', 'id': '5'},\n",
       "   {'displayName': 'Business', 'id': '6'},\n",
       "   {'displayName': 'Arts and Entertainment', 'id': '7'},\n",
       "   {'displayName': 'DROPPED', 'id': 'DROPPED'}]},\n",
       " 'precision': 0.16111112,\n",
       " 'f1ScoreAt1': 0.7526096,\n",
       " 'f1ScoreMacro': 0.23935229,\n",
       " 'f1Score': 0.27747175,\n",
       " 'falsePositiveCount': '4983',\n",
       " 'f1ScoreMicro': 0.27747175}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation['metrics']['confidenceMetrics'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3bf74363-7031-4b7d-9b3f-a73d74ec9be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review this model in the console:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west3/models/2095625182075944960/versions/1/evaluations/4587355914663221019?project=gcp-ccai-auto-ml-contactcenter\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review this model in the console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model.name}/versions/{model.version_id}/evaluations/{evaluation['name'].split('/')[-1]}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "184d1af6-ef6d-4907-acc0-ce45c2c62145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label =  Technology  has Predicted labels =  [13.0, 1.0, 1.0, 3.0, 1.0, 0.0, 5.0, 7.0, 0.0]\n",
      "True Label =  Sports  has Predicted labels =  [0.0, 76.0, 0.0, 0.0, 4.0, 0.0, 3.0, 8.0, 0.0]\n",
      "True Label =  Science  has Predicted labels =  [0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 4.0, 0.0]\n",
      "True Label =  Politics  has Predicted labels =  [2.0, 1.0, 0.0, 247.0, 0.0, 4.0, 19.0, 16.0, 0.0]\n",
      "True Label =  Health  has Predicted labels =  [0.0, 2.0, 1.0, 1.0, 40.0, 0.0, 4.0, 8.0, 0.0]\n",
      "True Label =  Environment  has Predicted labels =  [0.0, 0.0, 1.0, 0.0, 0.0, 9.0, 0.0, 0.0, 0.0]\n",
      "True Label =  Business  has Predicted labels =  [1.0, 3.0, 0.0, 49.0, 0.0, 5.0, 101.0, 26.0, 0.0]\n",
      "True Label =  Arts and Entertainment  has Predicted labels =  [0.0, 7.0, 0.0, 24.0, 8.0, 2.0, 14.0, 233.0, 0.0]\n",
      "True Label =  DROPPED  has Predicted labels =  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(evaluation['metrics']['confusionMatrix']['annotationSpecs'])):\n",
    "    print('True Label = ', evaluation['metrics']['confusionMatrix']['annotationSpecs'][i]['displayName'], ' has Predicted labels = ', evaluation['metrics']['confusionMatrix']['rows'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dd00758c-5200-4bf0-8a37-c27d9dfff5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_client = aip.gapic.ModelServiceClient(\n",
    "    client_options = {\n",
    "        'api_endpoint' : f'{REGION}-aiplatform.googleapis.com'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35305676-d2cd-4408-8a28-93742cde4f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slices = model_client.list_model_evaluation_slices(parent = evaluation['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21a901f5-9df2-4525-a3f1-29792002370a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  Science has logLoss =  0.04694494\n",
      "Label =  Arts and Entertainment has logLoss =  0.33384135\n",
      "Label =  Business has logLoss =  0.3318977\n",
      "Label =  Politics has logLoss =  0.26495764\n",
      "Label =  Sports has logLoss =  0.101992965\n",
      "Label =  Health has logLoss =  0.08870014\n",
      "Label =  Technology has logLoss =  0.093657166\n",
      "Label =  Environment has logLoss =  0.037851635\n"
     ]
    }
   ],
   "source": [
    "for slice in slices:\n",
    "    print('Label = ', slice.slice_.value, 'has logLoss = ', slice.metrics['logLoss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd09bf-33db-4846-99eb-deb2e96baf7f",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abe812-c7df-4fc4-92a3-498fa3819ea6",
   "metadata": {},
   "source": [
    "### Prepare a record for prediction: instance and parameters lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3736b438-61a2-42c1-8563-1f3345460649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uri,url,date,body,time,dateTime,dateTimePub,lang,isDuplicate,dataType,sentiment,eventUri,image,sharesFacebook,sourceLocationLabel,categoryLabels,categoryWeights,alexaCountryRank,date_column,year,year_month,num_documents,PERSON,OTHER,ORGANIZATION,EVENT,LOCATION,WORK_OF_ART,CONSUMER_GOOD,NUMBER,DATE,NUMBER_mean_salience,DATE_mean_salience,PRICE,ADDRESS,ADDRESS_mean_salience,PHONE_NUMBER,PHONE_NUMBER_mean_salience'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAR_OMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c8321cfa-3a09-4820-80cb-f81d3112c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uri,url,date,body,time,dateTime,dateTimePub,lang,isDuplicate,dataType,sentiment,eventUri,image,sharesFacebook,sourceLocationLabel,categoryLabels,categoryWeights,alexaCountryRank,date_column,year,year_month,num_documents,PERSON,OTHER,ORGANIZATION,EVENT,LOCATION,WORK_OF_ART,CONSUMER_GOOD,NUMBER,NUMBER_mean_salience,DATE_mean_salience,PRICE,ADDRESS,ADDRESS_mean_salience,PHONE_NUMBER,PHONE_NUMBER_mean_salience\n"
     ]
    }
   ],
   "source": [
    "# Remove the duplicate date from the string\n",
    "VAR_OMIT_rev = VAR_OMIT.replace(\"DATE,\", \"\")\n",
    "\n",
    "print(VAR_OMIT_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "870712bb-215a-4c84-b735-01aa84f7bf2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = bq.query(\n",
    "    query = f\"\"\"\n",
    "        SELECT * EXCEPT({VAR_TARGET}, split, {VAR_OMIT_rev})\n",
    "        FROM {DATANAME}.{TABLE_ID}\n",
    "        WHERE split='TEST'\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    ").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a53b339c-1b16-4ee6-9cb5-2b8d052e627d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'relevance', 'authors', 'sourceTitle', 'importanceRank',\n",
       "       'alexaGlobalRank', 'month', 'shares_scaled', 'body_pre', 'score',\n",
       "       'magnitude', 'Toxic', 'Insult', 'Profanity', 'Derogatory', 'Sexual',\n",
       "       'Death_Harm__Tragedy', 'Violent', 'Firearms__Weapons', 'Public_Safety',\n",
       "       'Health', 'Religion__Belief', 'Illicit_Drugs', 'War__Conflict',\n",
       "       'Politics', 'Finance', 'Legal', 'PERSON_mean_salience',\n",
       "       'OTHER_mean_salience', 'ORGANIZATION_mean_salience',\n",
       "       'EVENT_mean_salience', 'LOCATION_mean_salience',\n",
       "       'WORK_OF_ART_mean_salience', 'CONSUMER_GOOD_mean_salience',\n",
       "       'PRICE_mean_salience'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca306f86-0b1c-401c-933c-dbf2762cfd7b",
   "metadata": {},
   "source": [
    "Change the columns type for the autoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f4fde4c4-c0ae-4cb4-9207-22be383bb7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'GF, Varrese sotto attacco: pesanti critiche da due ex Vipponi',\n",
       " 'relevance': '1',\n",
       " 'authors': 'Debora Manzoli',\n",
       " 'sourceTitle': 'libero.it',\n",
       " 'importanceRank': '1000000',\n",
       " 'alexaGlobalRank': '0',\n",
       " 'month': '12',\n",
       " 'shares_scaled': -0.28,\n",
       " 'body_pre': \"Scrittrice , copywriter , editor pubblicista mantovana , laureata Lettere , Cinema Tv . due libri all'attivo ama scrittura follia . L'ultima puntata Grande Fratello visto protagonisti Massimiliano Varrese , ripreso Alfonso Signorini brutti atteggiamenti confronti Beatrice Luzzi . molti web infatti richiesto squalifica Varrese , stanchi atteggiamenti troppo aggressivi , gieffino fine ' cavata solamente scuse Beatrice pubblico ramanzina presentatore . , però , andato giù proprio , due ex vipponi Grande Fratello , passato stati squalificati gioco , passati all'attacco . Ecco tratta cosa detto . Dopo ramanzina Alfonso Signorini diretta , Varrese ammesso propri errori fine ' cavata ( sempre resto ) semplice ammonizione . `` uomo devo far passare messaggi . Prometto me stesso lavorare cosa , confronti dell'opinione pubblica . Voglio mettere parte l'ascia guerra . vittoria qua dentro dimostrare fatti può lavorare aspetti '' , detto gieffino scuse Beatrice pubblico . momento , quindi , niente squalifica Varrese . decisione scatenato solo commenti contrari web , forti critiche parte due ex Vipponi edizioni passate invece stati squalificati senza troppi giri parole ( contrario Varrese ) . GF , due ex Vipponi Massimiliano Varrese : Ebbene sì , dopo dure critiche Roberta Bruzzone Rebecca Staffelli , os due ex Vipponi passate edizioni utilizzato social scagliarsi Massimiliano Varrese , Grande Fratello stesso , l'ennesima volta , deciso fargli trattamento favore ( concesso invece quando tempo vennero squalificati ) . primis parliamo Salvo Veneziano , espulso corso edizione alcune frasi molto gravi confronti Elisa De Panicis , commentato situazione scrivendo : `` ok. chiesto scusa . Pure chiesto scusa trattato criminale , infatti riguarda possono fallire '' . poi aggiunta critica Filippo Nardi , squalificato tempo frasi sessiste Maria Teresa Ruta . L'ex Vippone quindi utilizzato social dire `` salvataggio '' Varrese : `` ' storia programma stato cacciato messo gogna molto meno '' . finirà questione ? Signorini avvertito Varrese sottolineando prossimo comportamento reiterato nessuno potrà levargli squalifica . davvero così ? Vedremo .\",\n",
       " 'score': -0.10000000149011612,\n",
       " 'magnitude': 6.400000095367432,\n",
       " 'Toxic': 0.23918169736862183,\n",
       " 'Insult': 0.13278861343860626,\n",
       " 'Profanity': 0.05046677961945534,\n",
       " 'Derogatory': 0.016035471111536026,\n",
       " 'Sexual': 0.0024352301843464375,\n",
       " 'Death_Harm__Tragedy': 0.016105417162179947,\n",
       " 'Violent': 0.1867469847202301,\n",
       " 'Firearms__Weapons': 0.0,\n",
       " 'Public_Safety': 0.05181347206234932,\n",
       " 'Health': 0.002964426763355732,\n",
       " 'Religion__Belief': 0.011627906933426857,\n",
       " 'Illicit_Drugs': 0.009554140269756317,\n",
       " 'War__Conflict': 0.06603773683309555,\n",
       " 'Politics': 0.3313252925872803,\n",
       " 'Finance': 0.006836827844381332,\n",
       " 'Legal': 0.10094637423753738,\n",
       " 'PERSON_mean_salience': 0.017601417898929314,\n",
       " 'OTHER_mean_salience': 0.007805135486705694,\n",
       " 'ORGANIZATION_mean_salience': 0.0,\n",
       " 'EVENT_mean_salience': 0.006327095674350858,\n",
       " 'LOCATION_mean_salience': 0.0,\n",
       " 'WORK_OF_ART_mean_salience': 0.044574463119109474,\n",
       " 'CONSUMER_GOOD_mean_salience': 0.0,\n",
       " 'PRICE_mean_salience': 0.0}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['relevance'] = pred['relevance'].astype(str)\n",
    "pred['importanceRank'] = pred['importanceRank'].astype(str)\n",
    "pred['alexaGlobalRank'] = pred['alexaGlobalRank'].astype(str)\n",
    "pred['month'] = pred['month'].astype(str)\n",
    "\n",
    "newobs = pred.to_dict(orient='records')\n",
    "newobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b238f9e-b5af-4a21-904c-be3a31c7d371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instances = [json_format.ParseDict(newob, Value()) for newob in newobs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e71767-ebfd-4786-9038-de00447c7f69",
   "metadata": {},
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4d46f00e-6550-47c6-b403-c27da007059e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.models.Endpoint object at 0x7f81751723e0> \n",
       " resource name: projects/944308723981/locations/europe-west3/endpoints/1221042847017336832,\n",
       " <google.cloud.aiplatform.models.Endpoint object at 0x7f81751728f0> \n",
       " resource name: projects/944308723981/locations/europe-west3/endpoints/1242434945247346688,\n",
       " <google.cloud.aiplatform.models.Endpoint object at 0x7f81751722c0> \n",
       " resource name: projects/944308723981/locations/europe-west3/endpoints/6842661081882558464,\n",
       " <google.cloud.aiplatform.models.Endpoint object at 0x7f8174fa6f20> \n",
       " resource name: projects/944308723981/locations/europe-west3/endpoints/238132228343726080]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aip.Endpoint.list(filter=f'labels.notebook={NOTEBOOK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6969660d-c6eb-4e0d-b373-8c8cf91d62bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automl_datasetnlp_20240325224945'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint = aip.Endpoint.list(filter=f'labels.notebook={NOTEBOOK}')[0]\n",
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "24f4d61e-79cb-495b-9d6b-afebb93376c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': ['Technology',\n",
       "  'Sports',\n",
       "  'Science',\n",
       "  'Politics',\n",
       "  'Health',\n",
       "  'Environment',\n",
       "  'Business',\n",
       "  'Arts and Entertainment'],\n",
       " 'scores': [0.01056508254259825,\n",
       "  0.01481661759316921,\n",
       "  0.00365602714009583,\n",
       "  0.01545155979692936,\n",
       "  0.001204681699164212,\n",
       "  0.001032748143188655,\n",
       "  0.01098049152642488,\n",
       "  0.9422927498817444]}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances = instances) # or instances = newobs\n",
    "prediction.predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fb5af-ae44-4e4a-b31a-2fc86700fdf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dc25078c-29e7-4e14-82d5-73f2c6b0e2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": [newobs[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d6115734-c27b-493f-b6f9-927b91ec7940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"scores\": [\n",
      "        0.01056508533656597,\n",
      "        0.014816612936556339,\n",
      "        0.0036560278385877609,\n",
      "        0.015451557002961641,\n",
      "        0.001204681931994855,\n",
      "        0.0010327481431886549,\n",
      "        0.01098048780113459,\n",
      "        0.94229274988174438\n",
      "      ],\n",
      "      \"classes\": [\n",
      "        \"Technology\",\n",
      "        \"Sports\",\n",
      "        \"Science\",\n",
      "        \"Politics\",\n",
      "        \"Health\",\n",
      "        \"Environment\",\n",
      "        \"Business\",\n",
      "        \"Arts and Entertainment\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"deployedModelId\": \"1758308208813801472\",\n",
      "  \"model\": \"projects/944308723981/locations/europe-west3/models/2095625182075944960\",\n",
      "  \"modelDisplayName\": \"automl_datasetnlp_20240325224945\",\n",
      "  \"modelVersionId\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087c079-1e20-4225-aa7a-28f7dacce7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
