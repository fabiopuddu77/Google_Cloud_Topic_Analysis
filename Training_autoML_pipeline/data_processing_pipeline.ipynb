{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0696c69-f3dd-4a1f-b32a-4df7a7982737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "062cfb0b-b3fc-4aca-a129-30e5e0f460cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.44.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.16.0)\n",
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /opt/conda/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.17.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.25.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.17.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.11.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.15)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (26.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.18)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components) (3.1.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (2.1.3)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2023.11.17)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (69.0.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=14.3->google-cloud-aiplatform) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform  \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01280da6-f62d-414a-8b81-ab5c94090ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform as aip\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3505f3e4-7588-451f-b2fc-efc112b21d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ccai-storage/pipeline_root/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "REGION= \"europe-west3\"\n",
    "REPO_NAME = \"repo-demo3\"\n",
    "SERVICE_ACCOUNT = \"944308723981-compute@developer.gserviceaccount.com\"\n",
    "BUCKET_NAME = \"ccai-storage\"\n",
    "PIPELINE_NAME = \"processing_pipeline\"\n",
    "YAML_NAME = f\"{PIPELINE_NAME}.yml\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\"\n",
    "DISPLAY_NAME = PIPELINE_NAME.replace(\"_\", \"-\")\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591eb0ee-6b1d-4a54-99c0-223657c3757e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c33769-fd54-4964-ae3c-e9bff94ce38f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: Incorrect public member type for binding ccai-storage:\n",
      "CommandException: Incorrect public member type for binding ccai-storage:\n"
     ]
    }
   ],
   "source": [
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bca0b1-d93b-45c7-aa20-f7938478707e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=PIPELINE_ROOT, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904b81c9-68ce-4262-a9cf-4804a90ff456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_preparation:latest\")\n",
    "\n",
    "def data_preprocessing(\n",
    "    bucket_name: str,\n",
    "    file_path: str,\n",
    "    folder: str,\n",
    "    parquet_file_name: str, \n",
    "    processed_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    \n",
    "    from processing.data_preparation import GCSParquetLoader\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = GCSParquetLoader(bucket_name, file_path, folder, parquet_file_name)\n",
    "    processed_dataset.uri = processor.save_df_to_gcs_parquet()\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d14eed-dfa7-4cef-9a02-30f72aeee358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_tokenization:latest\")\n",
    "\n",
    "def data_tokenization(\n",
    "    bucket_name: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    tokenized_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.tokenization import TokenizationProcessor\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = TokenizationProcessor(bucket_name, file_path.uri, folder, parquet_file_name)\n",
    "    #processor.save_df_to_gcs_parquet()\n",
    "    \n",
    "    print(\"--\")\n",
    "    tokenized_dataset.uri = processor.save_df_to_gcs_parquet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28df6a21-a909-4935-98cf-8caefef488c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_sentiment:latest\")\n",
    "\n",
    "def data_sentiment(\n",
    "    bucket_name: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    text_column: str,\n",
    "    num_doc: int,\n",
    "    sentiment_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.sentiment import GCSSentimentAnalyzer\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = GCSSentimentAnalyzer(bucket_name, file_path.uri, folder, parquet_file_name, \n",
    "                                    text_column, num_doc)\n",
    "    \n",
    "    \n",
    "    print(\"--\")\n",
    "    sentiment_dataset.uri = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb9a275-96e5-47af-a287-2480771c088a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_moderate:latest\")\n",
    "\n",
    "def data_moderate(\n",
    "    bucket_name: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    text_column: str,\n",
    "    num_doc: int,\n",
    "    moderate_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.moderate import GCSTextModerationLoader\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    processor = GCSTextModerationLoader(bucket_name, file_path.uri, folder, parquet_file_name, \n",
    "                                    text_column, num_doc)\n",
    "    \n",
    "    print(\"--\")\n",
    "    moderate_dataset.uri = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a31d9a-d874-4631-8a10-3d4877911917",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_entities:latest\")\n",
    "\n",
    "def data_entities(\n",
    "    bucket_name: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    text_column: str,\n",
    "    num_doc: int,\n",
    "    entities_dataset: Output[Artifact]\n",
    "):  \n",
    "    import logging\n",
    "    from processing.entities import GCSCEntityAnalyzer\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    print(f\"------ file_path.uri = {file_path.uri}\")\n",
    "    processor = GCSCEntityAnalyzer(bucket_name, file_path.uri, folder, parquet_file_name, \n",
    "                                    text_column, num_doc)\n",
    "    \n",
    "    \n",
    "    entities_dataset.uri = processor.process()\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64cadd9-386a-4504-a16d-a3125f5430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/data_bigquery:latest\")\n",
    "\n",
    "def data_bigquery(\n",
    "    bucket_name: str,\n",
    "    file_path: Input[Artifact],\n",
    "    folder: str,\n",
    "    parquet_file_name: str,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    location: str,\n",
    "    bigquery_table: Output[Artifact]\n",
    "    \n",
    "):  \n",
    "    \n",
    "    import logging\n",
    "    from processing.bigquery import GCS_Bigquery\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    print(f\"------ file_path.uri = {file_path.uri}\")\n",
    "    print(\"--\")\n",
    "    processor = GCS_Bigquery(bucket_name, file_path.uri, folder, parquet_file_name, \n",
    "                         project_id, dataset_id, table_id, location)\n",
    "    \n",
    "    \n",
    "    bigquery_table.uri = processor.upload_dataframe_to_bigquery()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27256c8-6987-436d-86c1-98d562485d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=DISPLAY_NAME,\n",
    "    description=\"Data preprocessing\"\n",
    ")\n",
    "def pipeline():\n",
    "    import logging\n",
    "    bucket_name = 'ccai-storage'\n",
    "    file_path = 'fabio/articlesoutputv3.parquet'\n",
    "    folder = 'pipeline'\n",
    "    PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter' \n",
    "    dataset_id = \"datasetnlp\"\n",
    "    table_id = \"stepfinalbq\"\n",
    "    text_column = 'body_pre'\n",
    "    location = \"europe-west3\"\n",
    "    num_doc=10\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "    \n",
    "    output_processing = 'step1_pipeline.parquet'\n",
    "    output_tokenization = 'step2_pipeline.parquet'\n",
    "    output_sentiment = 'step3_pipeline.parquet'\n",
    "    output_moderate = 'step4_pipeline.parquet'\n",
    "    output_entities = 'step5_pipeline.parquet'\n",
    "    output_final = 'pipeline/step_final_bq.parquet'\n",
    "\n",
    "    \n",
    "    processing_op = data_preprocessing(bucket_name=bucket_name, \n",
    "                                       file_path=file_path, \n",
    "                                       folder=folder, \n",
    "                                       parquet_file_name=output_processing)\n",
    "    \n",
    "    tokenization_op = data_tokenization(bucket_name=bucket_name, \n",
    "                                        file_path=processing_op.outputs[\"processed_dataset\"], \n",
    "                                        folder=folder, \n",
    "                                        parquet_file_name=output_tokenization)\n",
    "                                        \n",
    "    sentiment_op = data_sentiment(bucket_name=bucket_name, \n",
    "                                  file_path=tokenization_op.outputs[\"tokenized_dataset\"], \n",
    "                                  folder=folder, \n",
    "                                  parquet_file_name=output_sentiment,\n",
    "                                  text_column=text_column, num_doc=num_doc)\n",
    "    \n",
    "    moderate_op = data_moderate(bucket_name=bucket_name, \n",
    "                                    file_path=sentiment_op.outputs[\"sentiment_dataset\"], \n",
    "                                    folder=folder, \n",
    "                                    parquet_file_name=output_moderate,\n",
    "                                    text_column=text_column, num_doc=num_doc)\n",
    "        \n",
    "    entities_op = data_entities(bucket_name=bucket_name, \n",
    "                                file_path=moderate_op.outputs[\"moderate_dataset\"], \n",
    "                                folder=folder, \n",
    "                                parquet_file_name=output_entities,\n",
    "                                text_column=text_column, num_doc=num_doc)\n",
    "    \n",
    "    bigquery_op = data_bigquery(bucket_name=bucket_name, \n",
    "                                file_path=entities_op.outputs['entities_dataset'], \n",
    "                                folder=folder, \n",
    "                                parquet_file_name=output_final, \n",
    "                                project_id=PROJECT_ID, \n",
    "                                dataset_id=dataset_id, \n",
    "                                table_id=table_id,\n",
    "                                location=location)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab26547-fb48-4c65-8fc6-ed3a97bec0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Creating PipelineJob\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/944308723981/locations/europe-west3/pipelineJobs/processing-pipeline-20240323014813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: PipelineJob created. Resource name: projects/944308723981/locations/europe-west3/pipelineJobs/processing-pipeline-20240323014813\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: To use this PipelineJob in another session:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/944308723981/locations/europe-west3/pipelineJobs/processing-pipeline-20240323014813')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: pipeline_job = aiplatform.PipelineJob.get('projects/944308723981/locations/europe-west3/pipelineJobs/processing-pipeline-20240323014813')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west3/pipelines/runs/processing-pipeline-20240323014813?project=944308723981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west3/pipelines/runs/processing-pipeline-20240323014813?project=944308723981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=YAML_NAME\n",
    ")\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=YAML_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=,\n",
    ")\n",
    "\n",
    "job.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3449110-57e6-47aa-af66-328aacec7d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
