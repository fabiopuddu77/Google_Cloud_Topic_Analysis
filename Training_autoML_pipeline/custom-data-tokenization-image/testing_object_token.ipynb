{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7d6796-e915-4a99-88d0-322e8187b497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from processing.tokenization import TokenizationProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2482d5-3c5b-4e60-991f-b075c6799fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ccai-storage/pipeline_root/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "REGION= \"europe-west3\"\n",
    "REPO_NAME = \"repo-demo3\"\n",
    "SERVICE_ACCOUNT = \"944308723981-compute@developer.gserviceaccount.com\"\n",
    "BUCKET = \"ccai-storage\"\n",
    "PIPELINE_NAME = \"automl_pipeline\"\n",
    "YAML_NAME = f\"{PIPELINE_NAME}.yml\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline_root/\"\n",
    "DISPLAY_NAME = PIPELINE_NAME.replace(\"_\", \"-\")\n",
    "NOTEBOOK = \"automl\"\n",
    "DATANAME = \"datasetnlp\"\n",
    "BQ_NAME = \"finaldf5\"\n",
    "FILE_PATH = 'articlesoutput.parquet'\n",
    "FOLDER = 'pipeline'\n",
    "PROJECT_ID = 'gcp-ccai-auto-ml-contactcenter'\n",
    "TABLE_ID = \"stepfinalbq\"\n",
    "TEXT_COLUMN = 'body_pre'\n",
    "LOCATION = \"europe-west3\"\n",
    "NUM_DOC = 10\n",
    "#BQ_SOURCE = \"bq://gcp-ccai-auto-ml-contactcenter.datasetnlp.stepfinalbq\"\n",
    "OUTPUT_PROCESSING = 'step1_pipeline.parquet'\n",
    "OUTPUT_TOKENIZATION = 'step2_pipeline.parquet'\n",
    "OUTPUT_SENTIMENT = 'step3_pipeline.parquet'\n",
    "OUTPUT_MODERATE = 'step4_pipeline.parquet'\n",
    "OUTPUT_ENTITIES = 'step5_pipeline.parquet'\n",
    "OUTPUT_FINAL = 'pipeline/step_final_bq.parquet'\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc37bb27-e1f0-45d5-9f37-423ff038856b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO: Check if the file in exists in gcs step2_pipeline.parquet\n",
      "\n",
      "INFO: Start loading DataFrame from step1_pipeline.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the file in exists in gcs\n",
      "Start loading DataFrame from step1_pipeline.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded DataFrame from step1_pipeline.parquet\n",
      "\n",
      "INFO: Loading Data Tokenized\n",
      "\n",
      "INFO: Starting Tokenization DataFrame from step1_pipeline.parquet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame from step1_pipeline.parquet\n",
      "Loading Data Tokenized\n",
      "Starting Tokenization DataFrame from step1_pipeline.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Dataframe tokenized - process completed and file saved to GCS.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe tokenized - process completed and file saved to GCS.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'step2_pipeline.parquet'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s\\n')\n",
    "\n",
    "processor=TokenizationProcessor(bucket=BUCKET, file_path=OUTPUT_PROCESSING, \n",
    "                               folder=FOLDER, parquet_file_name=OUTPUT_TOKENIZATION,)\n",
    "processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ec9b6-ba33-46c2-a821-ca7a1e1a7e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
