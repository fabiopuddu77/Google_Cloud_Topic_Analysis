{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222f743-616c-474f-9a67-981037409df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs_parquet(df, bucket_name, folder, destination_blob_name):\n",
    "    \"\"\"Save a DataFrame to a Parquet file in GCS.\"\"\"\n",
    "    parquet_byte_stream = BytesIO()\n",
    "    df.to_parquet(parquet_byte_stream, index=False, engine='pyarrow')\n",
    "    parquet_byte_stream.seek(0)\n",
    "    \n",
    "    path = folder + \"/\" + destination_blob_name\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    \n",
    "    blob.upload_from_file(parquet_byte_stream, content_type='application/octet-stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6042c21-b7bf-4749-8bb8-9e1614ae8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a Parquet file from GCS and load it into a pandas DataFrame\n",
    "def load_parquet_from_gcs(bucket_name, file_path):\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    byte_stream = BytesIO()\n",
    "    blob.download_to_file(byte_stream)\n",
    "    byte_stream.seek(0)\n",
    "    df = pd.read_parquet(byte_stream, engine='pyarrow')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a74b4-043b-414c-b633-254d03e595ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv \n",
    "\n",
    "def upload_dataframe_to_bigquery(dataframe: pd.DataFrame, table_id: str, project_id: str, dataset_id: str, bucket_name: str, location: str) -> None:\n",
    "    # Create a BigQuery client object\n",
    "    client = bigquery.Client(project=project_id, location=location)  # Change location if necessary\n",
    "\n",
    "    # Convert pandas DataFrame to CSV\n",
    "    csv_filename = \"temp_data.csv\"\n",
    "    dataframe.to_csv(csv_filename, index=False, sep = '|')  # Quote all fields to ensure proper handling of special characters\n",
    "\n",
    "    try:\n",
    "        # Define the Google Cloud Storage (GCS) URI for the CSV file\n",
    "        gcs_uri = f\"gs://{bucket_name}/{csv_filename}\"\n",
    "\n",
    "        # Define the BigQuery table ID\n",
    "        bq_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "        # Define the job configuration for loading the CSV into BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,\n",
    "            allow_quoted_newlines=True,\n",
    "            field_delimiter=\"|\",\n",
    "            autodetect=True  # Automatically detect schema from CSV\n",
    "        )\n",
    "        \n",
    "        # Replace the file CSV\n",
    "        job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "        # Load the CSV file into BigQuery\n",
    "        with open(csv_filename, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(\n",
    "                source_file,\n",
    "                bq_table_id,\n",
    "                job_config=job_config,\n",
    "            )\n",
    "\n",
    "        # Wait for the job to complete\n",
    "        job.result()\n",
    "\n",
    "        # Get the destination table\n",
    "        destination_table = client.get_table(bq_table_id)\n",
    "\n",
    "        # Print the number of rows loaded\n",
    "        print(f\"Loaded {destination_table.num_rows} rows into BigQuery table: {bq_table_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading DataFrame to BigQuery: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up: delete the temporary CSV file\n",
    "        if os.path.exists(csv_filename):\n",
    "            os.remove(csv_filename)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
